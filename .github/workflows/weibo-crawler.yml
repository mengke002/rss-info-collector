name: Weibo RSS Crawler

on:
  schedule:
    # 北京时间 8:00-23:00 每1小时执行一次
    # UTC 时间 0:00-15:00 (北京时间 = UTC + 8)
    - cron: '0 0-15 * * *'  # 每小时整点，从UTC 0:00到15:00
  workflow_dispatch:  # 支持手动触发

jobs:
  crawl-weibo:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Weibo crawler
        run: |
          echo "========================================="
          echo "开始执行微博RSS爬取任务"
          echo "北京时间: $(TZ='Asia/Shanghai' date '+%Y-%m-%d %H:%M:%S')"
          echo "========================================="
          python3.12 main.py --task crawl --feed weibo --output json
        env:
          # 数据库配置
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_NAME: ${{ secrets.DB_NAME }}
          # 微博配置
          WEIBO_USER_IDS: ${{ secrets.WEIBO_USER_IDS }}
          RSSHUB_PREFIXES: ${{ secrets.RSSHUB_PREFIXES }}
          # 时区配置
          TZ: Asia/Shanghai

      - name: Display crawl results
        if: always()
        run: |
          echo "========================================="
          echo "任务执行完成"
          echo "北京时间: $(TZ='Asia/Shanghai' date '+%Y-%m-%d %H:%M:%S')"
          echo "========================================="
          if [ -f rss_crawler.log ]; then
            echo "最近的日志记录："
            tail -n 50 rss_crawler.log
          fi
